{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33dd86e6",
   "metadata": {},
   "source": [
    "# Introduction to LLMs using LangChain\n",
    "\n",
    "Since the release of [GPT-3.5](https://en.wikipedia.org/wiki/GPT-3#GPT-3.5) by [OpenAI](https://en.wikipedia.org/wiki/OpenAI) in 2022 \n",
    "generative AI and [large language models (LLM)](https://en.wikipedia.org/wiki/Large_language_model) have gained public attention. \n",
    "While different approaches exist for building custom applications based on LLMs, [LangChain](https://www.langchain.com) is used in this notebook. \n",
    "According to the documentation [LangChain](https://www.langchain.com) is\n",
    "\n",
    "> a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "The content of this notebook is based on the [LangChain quickstart](https://python.langchain.com/docs/get_started/quickstart/).\n",
    "\n",
    "## LangChain overview \n",
    "\n",
    "The goal of LangChain is to simplify the development of applications based on LLMs. This is achieved by providing tools for different aspects of \n",
    "the application lifecycle. For example, LangChain provides tools for the development, the deployment and the monitoring of LLM based \n",
    "applications. Typical use cases for LangChain are: \n",
    "- [Question answering using Retrieval augmented generation (RAG)](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [Chatbots](https://python.langchain.com/docs/use_cases/chatbots/)\n",
    "\n",
    "One of the nice feature of LangChain is the possibility to build applications based on different LLMs. Using LangChain is is \n",
    "possible to build application based on OpenAI GPT as well as on open source LLMs. In this notebook \n",
    "[Ollama](https://ollama.com/) will bs used as the basis for the applications.\n",
    "\n",
    "## Installing the necessary tools and libraries\n",
    "### LangChain\n",
    "The first step is to install the necessary tools and libraries. First, we install LangChain using the following command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25c26f",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "One helpful tool offered by LangChain for developing LLM-based applications is [LangSmith](https://smith.langchain.com/). While not \n",
    "necessary for this tutorial it is recommended to create a free LangChain account and setup LangSmith\n",
    "by setting the following environment variables. \n",
    "\n",
    "Note that the environment variables need to be set before Jupyter Lab is started. \n",
    "\n",
    "```zsh\n",
    "# MacOS & Linux\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=<your-api-key>\n",
    "\n",
    "# Windows or using the GUI\n",
    "setx LANGCHAIN_TRACING_V2 \"true\"\n",
    "setx LANGCHAIN_API_KEY <your-api-key>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7da09",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "As mentioned above the LLM used for this tutorial is Ollama. Ollama can be downloaded [here](https://ollama.com/download).\n",
    "\n",
    "Once Ollama is installed the server cna be started using:\n",
    "\n",
    "```zsh\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "LangChain as well as the Ollama CLI interact with this server. A short documentation on how to use the Ollama CLI can be found [here](https://github.com/ollama/ollama).\n",
    "The first thing we need to do is to download a model. In order to download the Llama 3 model execute the following command. Be aware, that each of these models is several GB large. \n",
    "\n",
    "```zsh\n",
    "ollama pull llama3\n",
    "```\n",
    "\n",
    "Once a model has been downloaded we can start building the first little application using it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e0325",
   "metadata": {},
   "source": [
    "## LangChain hello world\n",
    "\n",
    "A first hello world LLM application can now be build using just 3 lines of Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LangChain Ollama library\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Instantiate the Llama3 model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Ask the model a question\n",
    "question = \"What is the meaning of life?\"\n",
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a9820",
   "metadata": {},
   "source": [
    "## Fun with flags (LLMs)\n",
    "Using LangChain it is also possible to build more complex prompts. Furthermore, different elements can be linked together in a chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dae2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are Sheldon from the Big Bang Theory. The user is Howard Wolowitz. Answer the question of the user in typical Sheldon style.\"),\n",
    "    (\"user\", \"What do you think about {topic}?\")\n",
    "])\n",
    "\n",
    "# Create a output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Build a chain \n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Use the chain and provide the template variables\n",
    "print(chain.invoke({\"topic\": \"Prof. Christian Drumm of the FH Aachen\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d00cf2",
   "metadata": {},
   "source": [
    "After execute the last query check the LangChain Web site. In the default project you can use LangSmith to analyze the executed queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f305f56",
   "metadata": {},
   "source": [
    "## Answering questions about documents\n",
    "The next step is to use retrieval augmented generation to answer queries about documents. We will use the Web site [Semestertermine](https://www.fh-aachen.de/fachbereiche/wirtschaft/rund-ums-studium/semestertermine) of the FB7 as a basis. \n",
    "First we load the document using LangChain's WebBasedLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25762c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.fh-aachen.de/fachbereiche/wirtschaft/rund-ums-studium/semestertermine\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5f0fa",
   "metadata": {},
   "source": [
    "In order to being able to a answer questions about documents a special representation of these documents is needed. This representation is called an *embedding*. \n",
    "The [LangChain documentation](https://python.langchain.com/docs/modules/data_connection/text_embedding/)\n",
    "contains the following explanation of embeddings. \n",
    "\n",
    "> Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "These embeddings will be stored in a database. LangChain support different databases for storing of embedding. For this tutorial FAISS will be use. FAISS can be easily installed using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f95753",
   "metadata": {},
   "source": [
    "Once FAISS is installed we can create the embeddings for the document and store them in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e036d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create the Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# Create a splitter to split the document in smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Store the embeddings in the vector store\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc226e",
   "metadata": {},
   "source": [
    "Now that we have a suitable representation of the document we can create a chain to answer questions about the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e42bf284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Prüfungsphase im Sommersemester 2024 findet vom 15.7.-2.8.2024 statt.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Beantworte die folgende Frage nur mit dem zur Verfügung gestellten Kontext:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Frage: {input}\"\"\")\n",
    "\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"Wann findet im Sommersemester die Prüfungsphase statt?\"})\n",
    "print(response[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2dcb5",
   "metadata": {},
   "source": [
    "# References\n",
    "- [LangChain quickstart](https://python.langchain.com/docs/get_started/introduction/)\n",
    "- [LangChain API Reference](https://api.python.langchain.com/)\n",
    "- [Retrieval augmented generation (RAG) from scratch](https://python.langchain.com/docs/get_started/introduction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0d97a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
